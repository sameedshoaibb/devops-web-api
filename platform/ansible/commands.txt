# ============================================================================
# PART 1: LINUX & ANSIBLE — INFRASTRUCTURE SETUP
# Step-by-Step Guide
# ============================================================================

## ENVIRONMENT DETAILS
- Host OS: Ubuntu 24.04 (headless)
- Server IP: 20.199.137.36
- Local Machine: macOS
- SSH Key: ~/.ssh/oncare (private) / oncare.pub (public)

## PREREQUISITES

### 1. SSH Key Generation (Local Machine)
# Generate SSH key pair for secure server access
ssh-keygen -t ed25519 -f ~/.ssh/oncare -C "sameedshoaib@Sameeds-MBP.fritz.box"

# Copy public key to the server
ssh-copy-id -i ~/.ssh/oncare.pub sameed@20.199.137.36

# OR manually add to server's ~/.ssh/authorized_keys:
# ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIL1y/P0+p5+LMCqfH6SSHGQ7xOjLQBrW6Fqvzsvq7Do4 sameedshoaib@Sameeds-MBP.fritz.box

### 2. Install Ansible (Local Machine)
pip3 install ansible

# Verify installation
ansible --version

# ============================================================================
# ANSIBLE CONFIGURATION
# ============================================================================

## STEP 1: Configure Inventory (inventory.ini)
# Define the target server and connection details

cat > inventory.ini << 'EOF'
# Simple Ansible Inventory
[servers]
sameed ansible_host=20.199.137.36 ansible_user=sameed ansible_ssh_private_key_file=~/.ssh/oncare

[all:vars]
ansible_python_interpreter=/usr/bin/python3
EOF

## STEP 2: Configure Variables (group_vars/all.yml)
# Define variables used by the playbook

cat > group_vars/all.yml << 'EOF'
---
# Configuration Variables

# User configuration
devops_user: devops
ssh_public_key: "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIL1y/P0+p5+LMCqfH6SSHGQ7xOjLQBrW6Fqvzsvq7Do4 sameedshoaib@Sameeds-MBP.fritz.box"

# Firewall ports
firewall_ports:
  - 22    # SSH
  - 80    # HTTP
  - 443   # HTTPS
  - 6443  # Kubernetes API
  - 8080  # Jenkins
  - 10250 # Kubelet
  - 8888  # Docker Port (NGINX test)
EOF

## STEP 3: Configure Ansible Settings (ansible.cfg)
cat > ansible.cfg << 'EOF'
[defaults]
inventory = inventory.ini
host_key_checking = False
stdout_callback = ansible.builtin.default
result_format = yaml

[privilege_escalation]
become = True
become_method = sudo
EOF

# ============================================================================
# TEST CONNECTIVITY
# ============================================================================

## STEP 4: Test Ansible Connection
cd /Users/sameedshoaib/Documents/Projects/oncare-devops-task/ansible
ansible all -m ping

# Expected Output:
# sameed | SUCCESS => {
#     "changed": false,
#     "ping": "pong"
# }

# ============================================================================
# RUN THE PLAYBOOK
# ============================================================================

## STEP 5: Execute Infrastructure Setup
ansible-playbook setup.yml

# For verbose output:
ansible-playbook setup.yml -v

# For very verbose (debugging):
ansible-playbook setup.yml -vvv

# ============================================================================
# WHAT THE PLAYBOOK DOES (setup.yml)
# ============================================================================

# The playbook performs the following tasks:

# 1. SYSTEM UPDATE
#    - Updates apt cache
#    - Upgrades all packages
#    - Installs essential packages (curl, gnupg, etc.)

# 2. CREATE NON-ROOT USER
#    - Creates 'devops' user with sudo privileges
#    - Adds SSH public key for passwordless login
#    - Configures passwordless sudo

# 3. INSTALL DOCKER
#    - Adds Docker GPG key and repository
#    - Installs docker-ce, docker-ce-cli, containerd.io
#    - Adds users (devops, ubuntu, jenkins) to docker group
#    - Enables Docker service

# 4. INSTALL KUBERNETES TOOLS
#    - Adds Kubernetes repository (v1.28)
#    - Installs kubelet, kubeadm, kubectl
#    - Holds packages to prevent auto-updates

# 5. INSTALL HELM
#    - Downloads official Helm install script
#    - Installs Helm 3

# 6. INSTALL JENKINS
#    - Adds Jenkins repository
#    - Installs Java 17 (dependency)
#    - Installs and enables Jenkins
#    - Retrieves initial admin password

# 7. CONFIGURE FIREWALL (UFW)
#    - Installs UFW
#    - Opens required ports (22, 80, 443, 6443, 8080, 10250, 8888)
#    - Enables firewall

# 8. DISABLE SWAP
#    - Runs swapoff -a
#    - Removes swap entries from /etc/fstab
#    - (Required for Kubernetes)

# 9. DEPLOY TEST NGINX CONTAINER
#    - Pulls nginx:alpine image
#    - Runs container on port 8888
#    - Verifies accessibility

# ============================================================================
# VERIFY INSTALLATION
# ============================================================================

## STEP 6: Verify Services on Remote Server
ssh -i ~/.ssh/oncare sameed@20.199.137.36

# Check Docker
docker --version
# Output: Docker version 29.1.3, build f52814d

# Check Kubernetes tools
kubectl version --client
# Output: Client Version: v1.28.15

kubeadm version
# Output: kubeadm version: &version.Info{Major:"1", Minor:"28"...

kubelet --version
# Output: Kubernetes v1.28.15

# Check Helm
helm version --short
# Output: v3.19.4+g7cfb6e4

# Check Jenkins status
sudo systemctl status jenkins
# Output: active (running)

# Check NGINX test container
sudo docker ps | grep nginx
# Output: nginx-test running on port 8888

# Check UFW status
sudo ufw status
# Output: Status: active with allowed ports

# Check swap is disabled
free -h | grep Swap
# Output: Swap: 0B 0B 0B

# Check devops user exists
id devops
# Output: uid=1001(devops) gid=1001(devops) groups=1001(devops),27(sudo),998(docker)

# ============================================================================
# USER CONFIGURATION
# ============================================================================

## Two Users Explained:
# 
# 1. sameed - Personal admin user
#    - Used by Ansible to connect to server
#    - Your SSH key already configured
#    - For interactive administration
#
# 2. devops - Dedicated automation user
#    - Created by the playbook
#    - Has sudo privileges (passwordless)
#    - Has Docker access
#    - For CI/CD pipelines and automation

## To login as devops user:
ssh -i ~/.ssh/oncare devops@20.199.137.36

# ============================================================================
# ACCESS SERVICES
# ============================================================================

## Jenkins Web UI
# URL: http://20.199.137.36:8080
# Initial Admin Password: 4f2e7fdc059a476ba71fb6432488706e

## NGINX Test Page
# URL: http://20.199.137.36:8888
curl http://20.199.137.36:8888

# ============================================================================
# PROJECT STRUCTURE
# ============================================================================

# ansible/
# ├── ansible.cfg           # Ansible configuration settings
# ├── inventory.ini         # Target server(s) definition
# ├── setup.yml             # Main playbook (run this)
# ├── run.sh                # Helper script to run playbook
# ├── group_vars/
# │   └── all.yml           # Variables (user, SSH key, ports)
# ├── README.md             # Main documentation
# ├── QUICKSTART.md         # Quick start guide
# └── START_HERE.md         # Getting started guide

# ============================================================================
# PART 1 REQUIREMENTS CHECKLIST
# ============================================================================

# ✅ Update the environment                    → apt update/upgrade
# ✅ Install Docker                            → docker-ce v29.1.3
# ✅ Install kubectl                           → v1.28.15
# ✅ Install kubeadm                           → v1.28.15
# ✅ Install kubelet                           → v1.28.15
# ✅ Install Helm                              → v3.19.4
# ✅ Install Jenkins                           → Running on port 8080
# ✅ Non-root user with SSH key access         → devops user created
# ✅ Sudo privileges                           → Passwordless sudo configured
# ✅ Configure UFW                             → Enabled with required ports
# ✅ Disable swap                              → Swap disabled for K8s

# ============================================================================
# NEXT STEPS (Parts 2-5)
# ============================================================================

# Part 2: Docker - Containerize the banking-api application
# Part 3: Kubernetes + Helm - Deploy using Helm charts
# Part 4: Jenkins - Create CI/CD pipeline
# Part 5: Secrets Management - Configure Vault or K8s Secrets

# ============================================================================
# KUBERNETES CLUSTER INITIALIZATION
# ============================================================================

# NOTE: The Ansible playbook installs Kubernetes TOOLS (kubectl, kubeadm, kubelet)
# but does NOT initialize a cluster. The steps below initialize a single-node cluster.

# UNDERSTANDING KUBERNETES SETUP:
# ┌────────────────────────────────────────────────────────────────────────────┐
# │ 1. Ansible installs TOOLS (kubectl, kubeadm, kubelet)                      │
# │ 2. kubeadm init creates the CLUSTER (control plane components)             │
# │ 3. kubectl config enables USERS to communicate with the cluster            │
# │ 4. CNI plugin enables POD NETWORKING                                       │
# └────────────────────────────────────────────────────────────────────────────┘

## PREREQUISITES - Fix containerd and kernel settings (if needed)
# These may already be configured, but are required for kubeadm to work

# WHY: containerd's default config disables CRI, which Kubernetes needs
# Fix containerd CRI configuration
sudo rm -f /etc/containerd/config.toml
sudo systemctl restart containerd

# WHY: Kubernetes needs br_netfilter for iptables to see bridged traffic
# Load required kernel modules for Kubernetes networking
sudo modprobe br_netfilter
echo "br_netfilter" | sudo tee /etc/modules-load.d/k8s.conf

# WHY: These sysctl settings enable:
# - iptables to process bridged IPv4/IPv6 traffic (required for kube-proxy)
# - IP forwarding (required for pod-to-pod communication across nodes)
# Configure sysctl settings for Kubernetes
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF
sudo sysctl --system

# ============================================================================
# STEP 1: Initialize the Kubernetes Cluster
# ============================================================================

# IMPORTANT: Use the PRIVATE IP, not the public IP!
# - Public IP (20.199.137.36) = Azure NAT IP, VM cannot bind to it
# - Private IP (10.0.0.4) = Actual VM interface IP

# Get your private IP:
hostname -I
# Output: 10.0.0.4 172.17.0.1

# Initialize the cluster
# --pod-network-cidr=10.244.0.0/16 : CIDR for pod network (Flannel default)
#                                    This is SEPARATE from Azure VNet (10.0.0.0/16)
#                                    No overlap = no conflict
# --apiserver-advertise-address    : Use PRIVATE IP of the VM
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=10.0.0.4

# Expected output (save this!):
# Your Kubernetes control-plane has initialized successfully!
# ...
# kubeadm join 10.0.0.4:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>

# ============================================================================
# STEP 2: Configure kubectl for Users
# ============================================================================

# IMPORTANT: kubectl needs a config file to know how to connect to the cluster
# The admin.conf file contains credentials and cluster endpoint info
# Each user that needs kubectl access must have this file in their ~/.kube/config

# For sameed user (current user):
mkdir -p $HOME/.kube
sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# For devops user (automation user):
sudo mkdir -p /home/devops/.kube
sudo cp -f /etc/kubernetes/admin.conf /home/devops/.kube/config
sudo chown -R devops:devops /home/devops/.kube

# For root user (optional, for system-level operations):
sudo mkdir -p /root/.kube
sudo cp -f /etc/kubernetes/admin.conf /root/.kube/config

# WHY EACH USER NEEDS SEPARATE CONFIG:
# - kubectl looks for config at ~/.kube/config by default
# - Each user has different $HOME directory
# - sameed: /home/sameed/.kube/config
# - devops: /home/devops/.kube/config  
# - root:   /root/.kube/config

# Verify kubectl works for each user:
kubectl get nodes                      # As current user (sameed)
sudo -u devops kubectl get nodes       # As devops user
sudo kubectl get nodes                 # As root user

# Output: vm-1   NotReady   control-plane   <age>   v1.28.15
# Note: Status is "NotReady" until CNI is installed

# ============================================================================
# STEP 3: Install CNI (Container Network Interface)
# ============================================================================

# WHY CNI IS NEEDED:
# - Kubernetes doesn't include networking by default
# - Pods need IP addresses and ability to communicate
# - CNI plugins provide this network layer
# - Without CNI: Node stays "NotReady", pods stuck in "Pending"

# Kubernetes requires a CNI plugin for pod networking
# We use Flannel (simple, works well for single-node clusters)
# Alternative: Calico (more features, better for production)

# WHY FLANNEL:
# - Simple to install and configure
# - Low resource overhead
# - Uses 10.244.0.0/16 by default (matches our --pod-network-cidr)
# - Good for learning and small clusters

# Install Flannel:
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

# Expected output:
# namespace/kube-flannel created
# clusterrole.rbac.authorization.k8s.io/flannel created
# clusterrolebinding.rbac.authorization.k8s.io/flannel created
# serviceaccount/flannel created
# configmap/kube-flannel-cfg created
# daemonset.apps/kube-flannel-ds created

# After installing CNI, node should become "Ready" within 30-60 seconds

# ============================================================================
# STEP 4: Allow Pods on Control Plane (Single-Node Cluster)
# ============================================================================

# WHY THIS IS NEEDED:
# - By default, Kubernetes "taints" control-plane nodes
# - Taints prevent regular workload pods from being scheduled there
# - This is a security/stability feature for production clusters
# - For single-node clusters, we MUST remove this taint or nothing can run

# WHAT IS A TAINT:
# - A taint is a label that repels pods
# - Control-plane taint: node-role.kubernetes.io/control-plane:NoSchedule
# - Pods without matching "toleration" won't be scheduled on tainted nodes

kubectl taint nodes --all node-role.kubernetes.io/control-plane-
# The trailing "-" means REMOVE the taint
# Output: node/vm-1 untainted

# PRODUCTION NOTE:
# In a multi-node cluster, you would NOT remove this taint
# Instead, you'd add worker nodes for running application workloads

# ============================================================================
# STEP 5: Verify Cluster is Running
# ============================================================================

# Check node status (should be "Ready" after CNI is installed):
kubectl get nodes
# Output:
# NAME   STATUS   ROLES           AGE     VERSION
# vm-1   Ready    control-plane   2m30s   v1.28.15

# Check all system pods are running:
kubectl get pods -A
# Expected output (all Running):
# NAMESPACE      NAME                           READY   STATUS    RESTARTS   AGE
# kube-flannel   kube-flannel-ds-xxxxx          1/1     Running   0          60s
# kube-system    coredns-xxxxx                  1/1     Running   0          2m
# kube-system    coredns-xxxxx                  1/1     Running   0          2m
# kube-system    etcd-vm-1                      1/1     Running   0          2m
# kube-system    kube-apiserver-vm-1            1/1     Running   0          2m
# kube-system    kube-controller-manager-vm-1   1/1     Running   0          2m
# kube-system    kube-proxy-xxxxx               1/1     Running   0          2m
# kube-system    kube-scheduler-vm-1            1/1     Running   0          2m

# Check cluster info:
kubectl cluster-info
# Output:
# Kubernetes control plane is running at https://10.0.0.4:6443
# CoreDNS is running at https://10.0.0.4:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

# ============================================================================
# KUBERNETES CLUSTER SUMMARY
# ============================================================================

# Cluster Details:
# - Control Plane IP: 10.0.0.4 (private)
# - Kubernetes Version: v1.28.15
# - CNI: Flannel
# - Pod Network CIDR: 10.244.0.0/16
# - Service CIDR: 10.96.0.0/12 (default)

# Network Architecture:
# ┌─────────────────────────────────────────────────────────────┐
# │ Azure VNet: 10.0.0.0/16                                     │
# │   └── VM Private IP: 10.0.0.4                               │
# │                                                             │
# │ Kubernetes Networks (internal only):                        │
# │   ├── Pod Network: 10.244.0.0/16 (Flannel)                  │
# │   └── Service Network: 10.96.0.0/12                         │
# │                                                             │
# │ Public Access: 20.199.137.36 (Azure NAT)                    │
# └─────────────────────────────────────────────────────────────┘

# ============================================================================
# TROUBLESHOOTING
# ============================================================================

# If kubeadm init fails, reset and try again:
sudo kubeadm reset -f
sudo rm -rf /etc/cni/net.d ~/.kube /home/devops/.kube /root/.kube /var/lib/etcd

# Clean iptables (be careful - may disconnect SSH temporarily):
sudo iptables -F
sudo iptables -t nat -F
sudo iptables -t mangle -F
sudo iptables -X

# Restart containerd before reinitializing:
sudo systemctl restart containerd

# Check kubelet logs:
sudo journalctl -xeu kubelet --no-pager | tail -50

# Check container runtime:
sudo crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a

# Regenerate join token (if needed for worker nodes):
kubeadm token create --print-join-command

# ============================================================================
# COMMON ISSUES AND SOLUTIONS
# ============================================================================

# ISSUE 1: "connection refused" error when running kubectl
# CAUSE: kube-apiserver is not running or crashing
# SOLUTION: Wait 30-60 seconds for API server to stabilize, or check logs:
#   sudo crictl logs $(sudo crictl ps -a | grep kube-apiserver | awk '{print $1}')

# ISSUE 2: kubectl works for one user but not another
# CAUSE: Missing or incorrect ~/.kube/config for that user
# SOLUTION: Copy admin.conf to the user's .kube directory:
#   sudo cp /etc/kubernetes/admin.conf /home/<user>/.kube/config
#   sudo chown -R <user>:<user> /home/<user>/.kube

# ISSUE 3: "The connection to the server was refused"
# CAUSE: API server binding to wrong IP or not started
# SOLUTION: Ensure you used PRIVATE IP (10.0.0.4) not PUBLIC IP (20.199.137.36)

# ISSUE 4: Pods stuck in ContainerCreating
# CAUSE: CNI not installed or misconfigured
# SOLUTION: Install Flannel CNI:
#   kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

# ISSUE 5: Control plane components keep restarting
# CAUSE: Resource issues or configuration problems
# SOLUTION: Check system resources and kubelet logs:
#   free -h                    # Check memory
#   df -h /                    # Check disk
#   sudo journalctl -xeu kubelet --no-pager | tail -100

# ============================================================================
# QUICK REFERENCE - KUBERNETES COMMANDS
# ============================================================================

# Cluster Information:
kubectl cluster-info                    # Show cluster endpoint
kubectl get nodes                       # List all nodes
kubectl get pods -A                     # List all pods in all namespaces

# Pod Management:
kubectl get pods                        # List pods in default namespace
kubectl get pods -n <namespace>         # List pods in specific namespace
kubectl describe pod <pod-name>         # Detailed pod info
kubectl logs <pod-name>                 # View pod logs

# Deployments:
kubectl get deployments                 # List deployments
kubectl apply -f <file.yaml>            # Apply configuration
kubectl delete -f <file.yaml>           # Delete configuration

# Debugging:
kubectl get events                      # View cluster events
kubectl top nodes                       # Node resource usage (requires metrics-server)
kubectl top pods                        # Pod resource usage (requires metrics-server)

# ============================================================================
# FINAL CLUSTER STATE
# ============================================================================

# After completing all steps, your cluster should show:
#
# $ kubectl get nodes
# NAME   STATUS   ROLES           AGE   VERSION
# vm-1   Ready    control-plane   5m    v1.28.15
#
# $ kubectl get pods -A
# NAMESPACE      NAME                           READY   STATUS    RESTARTS   AGE
# kube-flannel   kube-flannel-ds-xxxxx          1/1     Running   0          4m
# kube-system    coredns-xxxxx                  1/1     Running   0          5m
# kube-system    coredns-xxxxx                  1/1     Running   0          5m
# kube-system    etcd-vm-1                      1/1     Running   0          5m
# kube-system    kube-apiserver-vm-1            1/1     Running   0          5m
# kube-system    kube-controller-manager-vm-1   1/1     Running   0          5m
# kube-system    kube-proxy-xxxxx               1/1     Running   0          5m
# kube-system    kube-scheduler-vm-1            1/1     Running   0          5m
#
# All users (sameed, devops, root) can run kubectl commands

# ============================================================================
# END OF PART 1 - INFRASTRUCTURE SETUP COMPLETE
# ============================================================================